\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{italian}
\@writefile{toc}{\select@language{italian}}
\@writefile{lof}{\select@language{italian}}
\@writefile{lot}{\select@language{italian}}
\select@language{italian}
\@writefile{toc}{\select@language{italian}}
\@writefile{lof}{\select@language{italian}}
\@writefile{lot}{\select@language{italian}}
\citation{Samuel}
\citation{analogia}
\@writefile{toc}{\contentsline {section}{Introduzione}{3}{section*.2}}
\citation{NFL}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Algoritmi di apprendimento}{4}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Costruzione di un algoritmo}{4}{section.1.1}}
\newlabel{Costruzione}{{1.1}{4}{Costruzione di un algoritmo}{section.1.1}{}}
\citation{an}
\newlabel{train-error}{{1.1}{6}{Costruzione di un algoritmo}{equation.1.1.1}{}}
\newlabel{minimi}{{1.2}{6}{Costruzione di un algoritmo}{equation.1.1.2}{}}
\newlabel{eq:min}{{1.3}{6}{Costruzione di un algoritmo}{equation.1.1.3}{}}
\newlabel{test-error}{{1.4}{7}{Costruzione di un algoritmo}{equation.1.1.4}{}}
\newlabel{test}{{1.5}{7}{Costruzione di un algoritmo}{equation.1.1.5}{}}
\citation{errval}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Apprendimento supervisionato}{8}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Regressione}{9}{subsection.1.2.1}}
\newlabel{regressione}{{1.2.1}{9}{Regressione}{subsection.1.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Regressione lineare}{9}{section*.3}}
\citation{rinforzo}
\newlabel{a}{{1.9}{10}{Regressione lineare}{equation.1.2.9}{}}
\newlabel{b}{{1.10}{10}{Regressione lineare}{equation.1.2.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Apprendimento non supervisionato}{10}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Apprendimento per rinforzo}{11}{section.1.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Reti neurali artificiali}{12}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Modello non lineare di un neurone artificiale.}}{13}{figure.2.1}}
\newlabel{neurone}{{2.1}{13}{Modello non lineare di un neurone artificiale}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Funzioni di attivazione}{14}{section.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Grafici delle funzioni di attivazione pi\`{u} usate.}}{14}{figure.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{Sigmoid}{14}{section*.5}}
\@writefile{toc}{\contentsline {subsubsection}{tanh}{15}{section*.6}}
\@writefile{toc}{\contentsline {subsubsection}{ReLu}{15}{section*.7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Addestramento di una rete}{16}{section.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{Algoritmo di back propagation}{17}{section*.8}}
\newlabel{back-prop}{{2.2}{17}{Algoritmo di back propagation}{section*.8}{}}
\newlabel{eqbp0}{{2.1}{17}{Algoritmo di back propagation}{equation.2.2.1}{}}
\newlabel{eqbp1}{{2.2}{17}{Algoritmo di back propagation}{equation.2.2.2}{}}
\newlabel{eqbp2}{{2.3}{17}{Algoritmo di back propagation}{equation.2.2.3}{}}
\newlabel{eqbp3}{{2.8}{18}{Algoritmo di back propagation}{equation.2.2.8}{}}
\newlabel{eqbp4}{{2.11}{18}{Algoritmo di back propagation}{equation.2.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Rete neurale semplice costituita da 3 strati con: $w_j$ pesi, $b_j$ i valori soglia e C funzione di costo.}}{20}{figure.2.3}}
\newlabel{fig:vanish}{{2.3}{20}{Rete neurale semplice costituita da 3 strati con: $w_j$ pesi, $b_j$ i valori soglia e C funzione di costo}{figure.2.3}{}}
\newlabel{catena}{{2.15}{20}{Il problema del Vanish Gradient}{equation.2.2.15}{}}
\newlabel{eqstima}{{2.20}{21}{Il problema del Vanish Gradient}{equation.2.2.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Grafico della funzione $\sigma '(z)$}}{21}{figure.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Grafico di $ReLu'(z)$}}{22}{figure.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reti Deep Feed-forward}{23}{section.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces modello di rete deep feedforward con uno strato}}{23}{figure.2.6}}
\newlabel{rete}{{2.6}{23}{modello di rete deep feedforward con uno strato}{figure.2.6}{}}
\citation{wordnet}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}ImageNet}{24}{section.2.4}}
\newlabel{ImageNet}{{2.4}{24}{ImageNet}{section.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Reti Neurali Convoluzionali - CNNs}{25}{section.2.5}}
\newlabel{eqconvoluzione}{{2.23}{25}{Reti Neurali Convoluzionali - CNNs}{equation.2.5.23}{}}
\newlabel{eqconvoluzionediscreta}{{2.24}{26}{Reti Neurali Convoluzionali - CNNs}{equation.2.5.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Esempio di rappresentazione di un'immagine in formato RGB in una CNN.}}{27}{figure.2.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Struttura di una CNNs}{27}{subsection.2.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Ciascuno dei 4 neuroni a destra \`{e} connesso solo a 3 neuroni del livello precedente. I pesi sono condivisi (stesso colore stesso peso).}}{28}{figure.2.8}}
\@writefile{toc}{\contentsline {subsubsection}{Strati convoluzionali}{29}{section*.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Ogni future map ha il suo kernel che scorre per tutta la dimensione dell'input.}}{30}{figure.2.9}}
\@writefile{toc}{\contentsline {subsubsection}{Strati di pooling}{31}{section*.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Struttura generale di una rete neurale convoluzionale}}{32}{figure.2.10}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Classificazione}{33}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{classificazione}{{3}{33}{Classificazione}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}K-Nearest Neighbor}{33}{section.3.1}}
\newlabel{dens}{{3.1}{34}{K-Nearest Neighbor}{equation.3.1.1}{}}
\newlabel{binomiale}{{3.2}{34}{K-Nearest Neighbor}{equation.3.1.2}{}}
\newlabel{eqbin1}{{3.4}{34}{K-Nearest Neighbor}{equation.3.1.4}{}}
\newlabel{eqbin2}{{3.5}{34}{K-Nearest Neighbor}{equation.3.1.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}TensorFlow}{36}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Data Flow Graph del calcolo di $ReLu(Wx+b)$.}}{37}{figure.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Strutture di tensori n-dimensionali, per alcune n.}}{38}{figure.4.2}}
\bibstyle{unsrt}
\bibdata{bibliografia}
\bibcite{Samuel}{1}
\bibcite{analogia}{2}
\bibcite{NFL}{3}
\bibcite{an}{4}
\bibcite{errval}{5}
\bibcite{rinforzo}{6}
\bibcite{wordnet}{7}
\bibcite{deep}{8}
\bibcite{machine}{9}
\bibcite{imagenet}{10}
\citation{deep}
\citation{machine}
\citation{imagenet}
\@writefile{toc}{\contentsline {chapter}{Bibliografia}{42}{lstnumber.-2.46}}
