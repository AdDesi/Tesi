\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{italian}
\@writefile{toc}{\select@language{italian}}
\@writefile{lof}{\select@language{italian}}
\@writefile{lot}{\select@language{italian}}
\select@language{italian}
\@writefile{toc}{\select@language{italian}}
\@writefile{lof}{\select@language{italian}}
\@writefile{lot}{\select@language{italian}}
\citation{Samuel}
\citation{analogia}
\@writefile{toc}{\contentsline {section}{Introduzione}{3}{section*.2}}
\citation{NFL}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Algoritmi di apprendimento}{5}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Costruzione di un algoritmo}{5}{section.1.1}}
\newlabel{Costruzione}{{1.1}{5}{Costruzione di un algoritmo}{section.1.1}{}}
\citation{an}
\newlabel{train-error}{{1.1}{7}{Costruzione di un algoritmo}{equation.1.1.1}{}}
\newlabel{minimi}{{1.2}{7}{Costruzione di un algoritmo}{equation.1.1.2}{}}
\newlabel{eq:min}{{1.3}{7}{Costruzione di un algoritmo}{equation.1.1.3}{}}
\newlabel{test-error}{{1.4}{9}{Costruzione di un algoritmo}{equation.1.1.4}{}}
\newlabel{test}{{1.5}{9}{Costruzione di un algoritmo}{equation.1.1.5}{}}
\citation{errval}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Apprendimento supervisionato}{11}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Regressione}{11}{subsection.1.2.1}}
\newlabel{regressione}{{1.2.1}{11}{Regressione}{subsection.1.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Regressione lineare}{11}{section*.3}}
\newlabel{eqa}{{1.9}{12}{Regressione lineare}{equation.1.2.9}{}}
\newlabel{eqb}{{1.10}{12}{Regressione lineare}{equation.1.2.10}{}}
\newlabel{eqbeta}{{1.11}{12}{Regressione lineare}{equation.1.2.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Apprendimento non supervisionato}{13}{section.1.3}}
\citation{clustering}
\citation{rinforzo}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Apprendimento per rinforzo}{15}{section.1.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Classificazione}{17}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{classificazione}{{2}{17}{Classificazione}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}K-Nearest Neighbor}{17}{section.2.1}}
\newlabel{knn}{{2.1}{17}{K-Nearest Neighbor}{section.2.1}{}}
\newlabel{dens}{{2.1}{18}{K-Nearest Neighbor}{equation.2.1.1}{}}
\newlabel{binomiale}{{2.2}{18}{K-Nearest Neighbor}{equation.2.1.2}{}}
\newlabel{eqbin1}{{2.4}{18}{K-Nearest Neighbor}{equation.2.1.4}{}}
\newlabel{eqbin2}{{2.5}{18}{K-Nearest Neighbor}{equation.2.1.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Reti neurali artificiali}{20}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cap:reti}{{3}{20}{Reti neurali artificiali}{chapter.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Modello non lineare di un neurone artificiale.}}{21}{figure.3.1}}
\newlabel{neurone}{{3.1}{21}{Modello non lineare di un neurone artificiale}{figure.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Funzioni di attivazione}{22}{section.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Grafici delle funzioni di attivazione pi\`{u} usate.}}{22}{figure.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{Sigmoid}{22}{section*.5}}
\@writefile{toc}{\contentsline {subsubsection}{tanh}{23}{section*.6}}
\@writefile{toc}{\contentsline {subsubsection}{ReLu}{23}{section*.7}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Addestramento di una rete}{24}{section.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{Algoritmo di back propagation}{25}{section*.8}}
\newlabel{back-prop}{{3.2}{25}{Algoritmo di back propagation}{section*.8}{}}
\newlabel{eqbp0}{{3.1}{25}{Algoritmo di back propagation}{equation.3.2.1}{}}
\newlabel{eqbp1}{{3.2}{25}{Algoritmo di back propagation}{equation.3.2.2}{}}
\newlabel{eqbp2}{{3.3}{25}{Algoritmo di back propagation}{equation.3.2.3}{}}
\newlabel{eqbp3}{{3.8}{26}{Algoritmo di back propagation}{equation.3.2.8}{}}
\newlabel{eqbp4}{{3.11}{27}{Algoritmo di back propagation}{equation.3.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Rete neurale semplice costituita da 3 strati con: $w_j$ pesi, $b_j$ i valori soglia e C funzione di costo.}}{28}{figure.3.3}}
\newlabel{fig:vanish}{{3.3}{28}{Rete neurale semplice costituita da 3 strati con: $w_j$ pesi, $b_j$ i valori soglia e C funzione di costo}{figure.3.3}{}}
\newlabel{catena}{{3.15}{28}{Il problema del Vanish Gradient}{equation.3.2.15}{}}
\newlabel{eqstima}{{3.20}{29}{Il problema del Vanish Gradient}{equation.3.2.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Grafico della funzione $\sigma '(z)$}}{30}{figure.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Grafico di $ReLu'(z)$}}{31}{figure.3.5}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Reti Deep Feed-forward}{31}{section.3.3}}
\citation{wordnet}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces modello di rete deep feedforward con uno strato}}{32}{figure.3.6}}
\newlabel{rete}{{3.6}{32}{modello di rete deep feedforward con uno strato}{figure.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}ImageNet}{32}{section.3.4}}
\newlabel{ImageNet}{{3.4}{32}{ImageNet}{section.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Reti Neurali Convoluzionali - CNNs}{33}{section.3.5}}
\newlabel{eqconvoluzione}{{3.23}{34}{Reti Neurali Convoluzionali - CNNs}{equation.3.5.23}{}}
\newlabel{eqconvoluzionediscreta}{{3.24}{35}{Reti Neurali Convoluzionali - CNNs}{equation.3.5.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Struttura di una CNNs}{35}{subsection.3.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Esempio di rappresentazione di un'immagine in formato RGB in una CNN.}}{36}{figure.3.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Ciascuno dei 4 neuroni a destra \`{e} connesso solo a 3 neuroni del livello precedente. I pesi sono condivisi (stesso colore stesso peso).}}{37}{figure.3.8}}
\@writefile{toc}{\contentsline {subsubsection}{Strati convoluzionali}{37}{section*.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Ogni future map ha il suo kernel che scorre per tutta la dimensione dell'input.}}{39}{figure.3.9}}
\@writefile{toc}{\contentsline {subsubsection}{Strati di pooling}{40}{section*.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Struttura generale di una rete neurale convoluzionale}}{41}{figure.3.10}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Shazarch}{42}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}MobileNet}{44}{section.4.1}}
\newlabel{sez:mobileNet}{{4.1}{44}{MobileNet}{section.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Fattorizzazione di una convoluzione standard in una convoluzione in profondit \`{a} e una convoluzione puntale.}}{45}{figure.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Struttura dell'architettura mobileNet.}}{47}{figure.4.2}}
\newlabel{fig:mobileNet}{{4.2}{47}{Struttura dell'architettura mobileNet}{figure.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}TensorFlow}{48}{section.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Data Flow Graph del calcolo di $ReLu(Wx+b)$.}}{49}{figure.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Strutture di tensori n-dimensionali, per alcune n.}}{50}{figure.4.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Schermate catturate dovo aver fatto il test: a sinistra test dell'algoritmo KNN con batch size, a destra senza.}}{53}{figure.4.5}}
\bibstyle{unsrt}
\bibdata{bibliografia}
\bibcite{Samuel}{1}
\bibcite{analogia}{2}
\bibcite{NFL}{3}
\bibcite{an}{4}
\bibcite{errval}{5}
\bibcite{rinforzo}{6}
\bibcite{wordnet}{7}
\bibcite{deep}{8}
\bibcite{machine}{9}
\bibcite{imagenet}{10}
\citation{deep}
\citation{machine}
\citation{imagenet}
\@writefile{toc}{\contentsline {chapter}{Bibliografia}{55}{figure.4.5}}
