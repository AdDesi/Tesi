\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{italian}
\@writefile{toc}{\select@language{italian}}
\@writefile{lof}{\select@language{italian}}
\@writefile{lot}{\select@language{italian}}
\select@language{italian}
\@writefile{toc}{\select@language{italian}}
\@writefile{lof}{\select@language{italian}}
\@writefile{lot}{\select@language{italian}}
\citation{Samuel}
\citation{analogia}
\@writefile{toc}{\contentsline {section}{Introduzione}{4}{section*.2}}
\citation{NFL}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Algoritmi di apprendimento}{6}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Costruzione di un algoritmo}{6}{section.1.1}}
\newlabel{Costruzione}{{1.1}{6}{Costruzione di un algoritmo}{section.1.1}{}}
\citation{an}
\newlabel{train-error}{{1.1}{9}{Costruzione di un algoritmo}{equation.1.1.1}{}}
\newlabel{minimi}{{1.2}{9}{Costruzione di un algoritmo}{equation.1.1.2}{}}
\newlabel{eq:min}{{1.3}{9}{Costruzione di un algoritmo}{equation.1.1.3}{}}
\newlabel{test-error}{{1.4}{11}{Costruzione di un algoritmo}{equation.1.1.4}{}}
\citation{errval}
\newlabel{test}{{1.5}{12}{Costruzione di un algoritmo}{equation.1.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Apprendimento supervisionato}{13}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Regressione}{14}{subsection.1.2.1}}
\newlabel{regressione}{{1.2.1}{14}{Regressione}{subsection.1.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Regressione lineare}{14}{section*.3}}
\newlabel{eqa}{{1.9}{15}{Regressione lineare}{equation.1.2.9}{}}
\newlabel{eqb}{{1.10}{15}{Regressione lineare}{equation.1.2.10}{}}
\newlabel{eqbeta}{{1.11}{15}{Regressione lineare}{equation.1.2.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Classificazione}{16}{subsection.1.2.2}}
\newlabel{classificazione}{{1.2.2}{16}{Classificazione}{subsection.1.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{K-Nearest Neighbour}{16}{section*.5}}
\newlabel{knn}{{1.2.2}{16}{K-Nearest Neighbour}{section*.5}{}}
\newlabel{dens}{{1.16}{17}{K-Nearest Neighbour}{equation.1.2.16}{}}
\newlabel{binomiale}{{1.17}{17}{K-Nearest Neighbour}{equation.1.2.17}{}}
\newlabel{eqbin1}{{1.19}{17}{K-Nearest Neighbour}{equation.1.2.19}{}}
\newlabel{eqbin2}{{1.20}{17}{K-Nearest Neighbour}{equation.1.2.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Apprendimento non supervisionato}{19}{section.1.3}}
\citation{clustering}
\citation{rinforzo}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Apprendimento per rinforzo}{22}{section.1.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Reti neurali artificiali}{25}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cap:reti}{{2}{25}{Reti neurali artificiali}{chapter.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Modello non lineare di un neurone artificiale.}}{26}{figure.2.1}}
\newlabel{neurone}{{2.1}{26}{Modello non lineare di un neurone artificiale}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Funzioni di attivazione}{28}{section.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Grafici delle funzioni di attivazione pi\`{u} usate.}}{28}{figure.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{Sigmoid}{28}{section*.6}}
\@writefile{toc}{\contentsline {subsubsection}{tanh}{29}{section*.7}}
\@writefile{toc}{\contentsline {subsubsection}{ReLu}{29}{section*.8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Addestramento di una rete}{30}{section.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{Algoritmo di back propagation}{32}{section*.9}}
\newlabel{back-prop}{{2.2}{32}{Algoritmo di back propagation}{section*.9}{}}
\newlabel{eqbp0}{{2.1}{32}{Algoritmo di back propagation}{equation.2.2.1}{}}
\newlabel{eqbp1}{{2.2}{33}{Algoritmo di back propagation}{equation.2.2.2}{}}
\newlabel{eqbp2}{{2.3}{33}{Algoritmo di back propagation}{equation.2.2.3}{}}
\newlabel{eqbp3}{{2.8}{33}{Algoritmo di back propagation}{equation.2.2.8}{}}
\newlabel{eqbp4}{{2.11}{34}{Algoritmo di back propagation}{equation.2.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Rete neurale semplice costituita da 3 strati con: $w_j$ pesi, $b_j$ i valori soglia e C funzione di costo.}}{35}{figure.2.3}}
\newlabel{fig:vanish}{{2.3}{35}{Rete neurale semplice costituita da 3 strati con: $w_j$ pesi, $b_j$ i valori soglia e C funzione di costo}{figure.2.3}{}}
\newlabel{catena}{{2.15}{36}{Il problema del Vanish Gradient}{equation.2.2.15}{}}
\newlabel{eqstima}{{2.20}{37}{Il problema del Vanish Gradient}{equation.2.2.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Grafico della funzione $\sigma '(z)$}}{38}{figure.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Grafico di $ReLu'(z)$}}{39}{figure.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reti Deep Feed-forward}{40}{section.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces modello di rete deep feedforward con uno strato}}{40}{figure.2.6}}
\newlabel{rete}{{2.6}{40}{modello di rete deep feedforward con uno strato}{figure.2.6}{}}
\citation{wordnet}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}ImageNet}{41}{section.2.4}}
\newlabel{ImageNet}{{2.4}{41}{ImageNet}{section.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Reti Neurali Convoluzionali - CNNs}{43}{section.2.5}}
\newlabel{eqconvoluzione}{{2.23}{43}{Reti Neurali Convoluzionali - CNNs}{equation.2.5.23}{}}
\newlabel{eqconvoluzionediscreta}{{2.25}{45}{Reti Neurali Convoluzionali - CNNs}{equation.2.5.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Esempio di rappresentazione di un'immagine in formato RGB in una CNN.}}{46}{figure.2.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Struttura di una CNNs}{46}{subsection.2.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Ciascuno dei 4 neuroni a destra \`{e} connesso solo a 3 neuroni del livello precedente. I pesi sono condivisi (stesso colore stesso peso).}}{47}{figure.2.8}}
\@writefile{toc}{\contentsline {subsubsection}{Strati convoluzionali}{48}{section*.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Ogni future map ha il suo kernel che scorre per tutta la dimensione dell'input.}}{50}{figure.2.9}}
\@writefile{toc}{\contentsline {subsubsection}{Strati di pooling}{52}{section*.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Struttura generale di una rete neurale convoluzionale}}{54}{figure.2.10}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Shazarch}{55}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}MobileNet}{59}{section.3.1}}
\newlabel{sez:mobileNet}{{3.1}{59}{MobileNet}{section.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Fattorizzazione di una convoluzione standard in una convoluzione in profondit \`{a} e una convoluzione puntale.}}{60}{figure.3.1}}
\citation{mobileNet}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Struttura dell'architettura mobileNet.}}{63}{figure.3.2}}
\newlabel{fig:mobileNet}{{3.2}{63}{Struttura dell'architettura mobileNet}{figure.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}TensorFlow}{64}{section.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Data Flow Graph del calcolo di $ReLu(Wx+b)$.}}{66}{figure.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Strutture di tensori n-dimensionali, per alcune n.}}{67}{figure.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Esempio di dati provenienti dal dataset Mnist}}{68}{figure.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Classificazione con KNN}{68}{subsection.3.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Schermate catturate dovo aver fatto il test: a sinistra test dell'algoritmo KNN con batch size, a destra senza.}}{72}{figure.3.6}}
\citation{Augmentor}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Shazarch}{73}{subsection.3.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Esempio di foto dell'Arco di Settimio Severo ruotata a destra.}}{74}{figure.3.7}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}AndroidStudio}{80}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Shazarch}{82}{subsection.3.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Conclusioni}{84}{section.3.4}}
\bibstyle{unsrt}
\bibdata{bibliografia}
\bibcite{Samuel}{1}
\bibcite{analogia}{2}
\bibcite{NFL}{3}
\bibcite{an}{4}
\bibcite{errval}{5}
\bibcite{rinforzo}{6}
\bibcite{wordnet}{7}
\bibcite{deep}{8}
\bibcite{machine}{9}
\bibcite{imagenet}{10}
\@writefile{toc}{\contentsline {chapter}{Bibliografia}{86}{section.3.4}}
\citation{deep}
\citation{machine}
\citation{imagenet}
