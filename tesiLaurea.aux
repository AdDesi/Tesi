\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{italian}
\@writefile{toc}{\select@language{italian}}
\@writefile{lof}{\select@language{italian}}
\@writefile{lot}{\select@language{italian}}
\select@language{italian}
\@writefile{toc}{\select@language{italian}}
\@writefile{lof}{\select@language{italian}}
\@writefile{lot}{\select@language{italian}}
\citation{Samuel}
\citation{analogia}
\@writefile{toc}{\contentsline {section}{Introduzione}{3}{section*.2}}
\citation{NFL}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Algoritmi di apprendimento}{5}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Costruzione di un algoritmo}{5}{section.1.1}}
\newlabel{Costruzione}{{1.1}{5}{Costruzione di un algoritmo}{section.1.1}{}}
\citation{an}
\newlabel{train-error}{{1.1}{7}{Costruzione di un algoritmo}{equation.1.1.1}{}}
\newlabel{minimi}{{1.2}{7}{Costruzione di un algoritmo}{equation.1.1.2}{}}
\newlabel{eq:min}{{1.3}{7}{Costruzione di un algoritmo}{equation.1.1.3}{}}
\newlabel{test-error}{{1.4}{9}{Costruzione di un algoritmo}{equation.1.1.4}{}}
\newlabel{test}{{1.5}{9}{Costruzione di un algoritmo}{equation.1.1.5}{}}
\citation{errval}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Apprendimento supervisionato}{11}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Regressione}{11}{subsection.1.2.1}}
\newlabel{regressione}{{1.2.1}{11}{Regressione}{subsection.1.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Regressione lineare}{11}{section*.3}}
\newlabel{eqa}{{1.9}{12}{Regressione lineare}{equation.1.2.9}{}}
\newlabel{eqb}{{1.10}{12}{Regressione lineare}{equation.1.2.10}{}}
\newlabel{eqbeta}{{1.11}{12}{Regressione lineare}{equation.1.2.11}{}}
\citation{rinforzo}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Apprendimento non supervisionato}{13}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Apprendimento per rinforzo}{13}{section.1.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Reti neurali artificiali}{14}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Modello non lineare di un neurone artificiale.}}{15}{figure.2.1}}
\newlabel{neurone}{{2.1}{15}{Modello non lineare di un neurone artificiale}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Funzioni di attivazione}{16}{section.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Grafici delle funzioni di attivazione pi\`{u} usate.}}{16}{figure.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{Sigmoid}{16}{section*.5}}
\@writefile{toc}{\contentsline {subsubsection}{tanh}{17}{section*.6}}
\@writefile{toc}{\contentsline {subsubsection}{ReLu}{17}{section*.7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Addestramento di una rete}{18}{section.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{Algoritmo di back propagation}{19}{section*.8}}
\newlabel{back-prop}{{2.2}{19}{Algoritmo di back propagation}{section*.8}{}}
\newlabel{eqbp0}{{2.1}{19}{Algoritmo di back propagation}{equation.2.2.1}{}}
\newlabel{eqbp1}{{2.2}{19}{Algoritmo di back propagation}{equation.2.2.2}{}}
\newlabel{eqbp2}{{2.3}{19}{Algoritmo di back propagation}{equation.2.2.3}{}}
\newlabel{eqbp3}{{2.8}{20}{Algoritmo di back propagation}{equation.2.2.8}{}}
\newlabel{eqbp4}{{2.11}{20}{Algoritmo di back propagation}{equation.2.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Rete neurale semplice costituita da 3 strati con: $w_j$ pesi, $b_j$ i valori soglia e C funzione di costo.}}{22}{figure.2.3}}
\newlabel{fig:vanish}{{2.3}{22}{Rete neurale semplice costituita da 3 strati con: $w_j$ pesi, $b_j$ i valori soglia e C funzione di costo}{figure.2.3}{}}
\newlabel{catena}{{2.15}{22}{Il problema del Vanish Gradient}{equation.2.2.15}{}}
\newlabel{eqstima}{{2.20}{23}{Il problema del Vanish Gradient}{equation.2.2.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Grafico della funzione $\sigma '(z)$}}{23}{figure.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Grafico di $ReLu'(z)$}}{24}{figure.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reti Deep Feed-forward}{25}{section.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces modello di rete deep feedforward con uno strato}}{25}{figure.2.6}}
\newlabel{rete}{{2.6}{25}{modello di rete deep feedforward con uno strato}{figure.2.6}{}}
\citation{wordnet}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}ImageNet}{26}{section.2.4}}
\newlabel{ImageNet}{{2.4}{26}{ImageNet}{section.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Reti Neurali Convoluzionali - CNNs}{27}{section.2.5}}
\newlabel{eqconvoluzione}{{2.23}{27}{Reti Neurali Convoluzionali - CNNs}{equation.2.5.23}{}}
\newlabel{eqconvoluzionediscreta}{{2.24}{28}{Reti Neurali Convoluzionali - CNNs}{equation.2.5.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Esempio di rappresentazione di un'immagine in formato RGB in una CNN.}}{29}{figure.2.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Struttura di una CNNs}{29}{subsection.2.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Ciascuno dei 4 neuroni a destra \`{e} connesso solo a 3 neuroni del livello precedente. I pesi sono condivisi (stesso colore stesso peso).}}{30}{figure.2.8}}
\@writefile{toc}{\contentsline {subsubsection}{Strati convoluzionali}{31}{section*.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Ogni future map ha il suo kernel che scorre per tutta la dimensione dell'input.}}{32}{figure.2.9}}
\@writefile{toc}{\contentsline {subsubsection}{Strati di pooling}{33}{section*.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Struttura generale di una rete neurale convoluzionale}}{34}{figure.2.10}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Classificazione}{35}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{classificazione}{{3}{35}{Classificazione}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}K-Nearest Neighbor}{35}{section.3.1}}
\newlabel{knn}{{3.1}{35}{K-Nearest Neighbor}{section.3.1}{}}
\newlabel{dens}{{3.1}{36}{K-Nearest Neighbor}{equation.3.1.1}{}}
\newlabel{binomiale}{{3.2}{36}{K-Nearest Neighbor}{equation.3.1.2}{}}
\newlabel{eqbin1}{{3.4}{36}{K-Nearest Neighbor}{equation.3.1.4}{}}
\newlabel{eqbin2}{{3.5}{36}{K-Nearest Neighbor}{equation.3.1.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}TensorFlow}{38}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Data Flow Graph del calcolo di $ReLu(Wx+b)$.}}{39}{figure.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Strutture di tensori n-dimensionali, per alcune n.}}{40}{figure.4.2}}
\bibstyle{unsrt}
\bibdata{bibliografia}
\bibcite{Samuel}{1}
\bibcite{analogia}{2}
\bibcite{NFL}{3}
\bibcite{an}{4}
\bibcite{errval}{5}
\bibcite{rinforzo}{6}
\bibcite{wordnet}{7}
\bibcite{deep}{8}
\bibcite{machine}{9}
\bibcite{imagenet}{10}
\citation{deep}
\citation{machine}
\citation{imagenet}
\@writefile{toc}{\contentsline {chapter}{Bibliografia}{44}{lstnumber.-2.53}}
