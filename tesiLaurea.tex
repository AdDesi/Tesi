\documentclass[a4paper,12pt]{report}
\usepackage[italian]{babel}
\usepackage[latin1]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{empheq}
\usepackage[dvipdfm]{graphicx}
\usepackage{bmpsize}
\usepackage{hyperref}
\usepackage{breakurl}
\setcounter{tocdepth}{2}
\begin{document}

%\begin{figure}
%\centering
%\includegraphics[scale=0.15]{Logo.jpg}
%\end{figure}
\begin{center}
\textbf{UNIVERSIT\'{A} DEGLI STUDI "ROMA TRE"}\\
\vspace{0.3cm}
\textmd{\large Dipartimento di Matematica e Fisica}\\
\vspace{0.2cm}
\large{ Corso di Laurea Magistrale in Scienze Computazionali}\\	
\vspace{1.4cm}	
\textmd{\Large{Tesi di Laurea Magistrale}}\\
\vspace{1.6cm}
\LARGE{\LARGE{\textbf{Ricerca della topologia ottimale di un sistema di deep learning per identificazioni di oggetti architettonici}}}\\
\vspace{5cm}
\begin{tabular}{ccccccccccc}
	\large{Candidato}& & & & & & & & & & \large{Relatore}\\
	\large{Dèsirèe Adiutori} & & & & & & & & & &\large{Prof. Luciano Teresi}
\end{tabular}\\
\vspace{4cm}
\normalsize{Anno Accademico 2017/2018}\\
\normalsize{Luglio 2018}
\end{center}
\newpage
\tableofcontents
\newpage
\section*{Introduzione}
\addcontentsline{toc}{section}{Introduzione}
Dall'invenzione dei computer, l'uomo fa sempre più affidamento sulle macchine per risolvere problemi complessi di calcolo. Con l'aumentare delle prestazioni dei computer, man mano si sono sviluppati algoritmi di calcolo sempre più efficienti.
Nel 1959 l'ingegnere del MIT, Arthur Samuel coniò il termine \textit{"machine learning"}, descrivendo l'apprendimento automatico come un "campo di studio che dà ai computer la possibilità di apprendere senza essere programmati esplicitamente per farlo".\cite{Samuel} \\
Definiamo l'apprendimento automatico come un insieme di metodi in grado di rilevare automaticamente i modelli tramite dei dati e quindi utilizzare i modelli scoperti per prevedere i dati futuri o per eseguire altri tipi di processi decisionali in condizioni di incertezza. L'insegnamento alla macchina \`e, pertanto, imprescindibile dai dati. Generalmente, più dati si passano alla macchina, più può imparare. Per questo motivo con l'avvento di Internet, dagli anni '90 ad oggi il tema del "machine learning" \`e diventato sempre più attuale, la mole di dati reperibile dal web \`e cospicuo e ha permesso che questo campo sia esponenzialmente progredito.\\
Il \textit{"deep learning"} \`e un tipo particolare di machine learning, che riguarda l'emulazione di come gli esseri umani apprendono. Esso affronta i problemi del machine learning, rappresentando il mondo come una gerarchia di concetti annidati: ogni concetto \`e definito in relazione a concetti più semplici e le rappresentazioni astratte vengono calcolate in termini di concetti meno astratti. Il Deep Learning implica l'utilizzo di reti neurali artificiali (\textit{deep artificial neural networks}), algoritmi e sistemi computazionali, ispirati al cervello umano, per affrontare i problemi del Machine Learning.\\
L'analogia di Shehzad Noor Taus Priyo puòaiutare a capire meglio cosa siano le reti neurali:\\
"Immaginiamole come una serie di porte da oltrepassare, dove l'input \`e l'uomo che le deve oltrepassare e ogni volta che lo fa cambia qualcosa nel suo comportamento finch\'{e}, all'ultima porta oltrepassata, l'uomo \`e diventato una persona del tutto differente, rappresentando l'output di questo processo."\cite{analogia} \\
Questa tesi si focalizza su un problema %tipo
particolare di algoritmo di machine learning: la Classificazione (\textit{Classification}), in particolar modo della classificazione di immagini. L'obiettivo principale \`e trovare un'architettura ottimale per l'algoritmo che identifica le immagini di oggetti architettonici, per fare questo bisogna trovare la giusta topologia, la giusta profondità e la giusta larghezza di ogni livello della rete neurale.
 
\chapter{Algoritmi di apprendimento}
Gli algoritmi di machine learning sono solitamente divisi in tre tipi principali:
\begin{itemize}
\item Supervised learning (apprendimento supervisionato)
\item Unsupervised learning (apprendimento non supervisionato)
\item Reinforcement learning (apprendimento per rinforzo)
\end{itemize}
Quali usare? Perch\'{e} sceglierne uno piuttosto che un altro?\\
La scelta dell'algoritmo da utilizzare dipende dal tipo di dati di cui si dispone. Ma la scelta finale va fatta solo esclusivamente dopo aver testato l'algoritmo, e si sceglie in base a quello più performante: un insieme di ipotesi che funziona bene in un dominio, potrebbe funzionare male in un altro.\\
\textbf{Teorema del No Free Lunch \cite[Wolper,1996]{NFL}}\\
\textit{Non esiste una definizione universale di algoritmo "migliore".}
\section{Costruzione di un algoritmo di apprendimento}\label{Costruzione}
Per costruire un algoritmo di apprendimento bisogna avere:
\begin{itemize}
\item processi(\textit{task}), compiti che l'algoritmo deve eseguire;
\item misuratori di rendimento, rilevatori delle caratteristiche dei processi;
\item esperienze, quantità di dati dal quale imparare.
\end{itemize}
I processi di apprendimento automatico descrivono come il sistema dovrebbe elaborare un esempio.
\newtheorem{defin}{Definizione}
\begin{defin}
Un \textbf{esempio} \`e una raccolta di caratteristiche che sono state misurate quantitativamente da alcuni oggetti o eventi elaborati.
\end{defin}
Di solito, un esempio viene rappresentato da un vettore $x\in \mathbb{R}^{n}$, dove ogni elemento $x_{i}$ rappresenta una caratteristica.
Dato un processo si cerca di capire quale sia la caratteristica principale, sulla quale si deve misurare il suo rendimento.
Infine, dobbiamo dare all'algoritmo un'esperienza sulla quale apprendere, che \`e quella che lo classificherà in uno dei tre tipi principali.
Questa esperienza l'apprende dai \textit{dataset}: una collezione di esempi.\\
I dataset possono essere di vari tipi:
\begin{itemize}
\item insieme di addestramento ( \textit{training set})
\item insieme di prova ( \textit{test set})
\item insieme di validazione ( \textit{validation set})
\end{itemize}
L' \textbf{insieme di addestramento} \`e una parte dell'insieme di dati che vengono utilizzati per addestrare un sistema di apprendimento supervisionato. Da questo insieme, l'algoritmo deve costruire una funzione che capisca, dai parametri, quali caratteristiche descrivono le varie categorie.\\
L' \textbf{insieme di prova} \`e un insieme di dati che, con l'insieme di addestramento, forma una partizione del dataset di partenza. Questi nuovi dati vengono utilizzati per valutare l'apprendimento dell'algoritmo "addestrato".\\
L' \textbf{insieme di validazione} \`e usato in maniera analoga all'insieme di prova, ma dei dati inseriti per testare l'algoritmo già si conosce la risposta (una parte di essi può far parte dell'insieme di addestramento) e da questa si valuta se l'output ottenuto \`e ottimale o meno.\\
Questi tre insiemi possono essere contemporaneamente e la scelta della loro cardinalità non \`e universale: dipende dal tipo di problema che viene affrontato.\\
Vediamo ora come valutare l'efficienza di un algoritmo:
\begin{defin}
L'\textbf{errore di allenamento}(\textit{training error}) \`e una misura di errore che si può calcolare sul set di allenamento. Indica quanto l'algortimo sta apprendendo.
\end{defin}
\begin{defin}
La \textbf{generalizzazione} \`e la capacità di un algoritmo di essere ottimale in seguito ad un input proveniente dall' insieme di prova.
\end{defin}
\begin{defin}
L'\textbf{errore di generalizzazione}(\textit{generalization error}) \`e una misura di errore che si può calcolare sull'insieme di prova. Verifica se l'algoritmo ha imparato o solo memorizzato.
Esso viene detto anche errore di test (\textit{test error}).
\end{defin}
Ipotizziamo che tutti gli esempi siano eventi indipendenti e che tuti gli insiemi, in cui partizioniamo l'insieme di dati, hanno la stessa distribuzione di probabilità uniforme.
\begin{defin}
Una \textbf{funzione di perdita} (\textit{loss function}) $L(y,\hat{y})$ \`e una funzione che misura la distanza (o l'errore) tra i valori di output previsto $\hat{y}$ e i valori effettivi ${y}$.
\end{defin}
Si possono usare varie misure, ad esempio l'errore quadratico medio(MSE):
\begin{equation}\label{train-error}
L(y,\hat{y})_{train}=\frac{1}{n}\sum_{i=1}^{n}(\hat{y}_{(train)}-y_{(train)})_{i}^2
\end{equation}
La nostra funzione di predizione dipender\'{a} da dei parametri, rappresentati da un vettore $w$, lo scopo\`e di minimizzare l'errore di allenamento variando $w$.
In base al tipo di apprendimento e al problema da affrontare, verranno usati vari algoritmi per risolvere problemi di minimizzazione libera. Gli algoritmi che risolverono il problema:
\begin{equation}\label{minimi}
f(x^{*})=\min_{x\in \mathbb{R}^{n}}f(x),\qquad \qquad f\in C^{2}
\end{equation}
Per risolvere problemi di questo tipo, spesso viene utilizzato il metodo di discesa del gradiente.\cite{an}\\
La struttura generale di un metodo di discesa iterativo di minimizzazione \`e:
\begin{equation}\label{eq:min}
x_{k+1}=x_{k}+\beta_{k}d_{k}
\end{equation}
dove $x_{0}$ \`e assegnato, $\beta_{k}\in \mathbb{R^{+}}$ \`e il passo e $d_{k}\in \mathbb{R}^{n}$ \`e la direzione lungo la quale ci si muove che, essendo di discesa, sarà tale che: $(d_{k},\nabla f(x_{k}))<0$. Sia il passo che la direzione vanno scelti opportunamente ad ogni iterazione, in modo che $f(x_{k+1})<f(x_{k})$.\\
Per ogni passo imponiamo:
$$f(x_{k}+\beta_{k}d_{k})=\min_{\beta}\{f(x_{k}+\beta d_{k})\}\qquad \qquad (strategia\: \: di\: \: ricerca\: \: esatta)$$
Mentre per la direzione: $d_{k}=-\nabla f(x_{k})$. La derivata direzionale di $f$ nella direzione $d_{k}$ vale $$\frac{\partial f}{\partial d_{k}}(x_{k})=\frac{(d_{k},\nabla f(x_{k}))}{\parallel d_{k}\parallel}=-\parallel \nabla f(x_{k})\parallel$$ e per la disuguaglianza di Cauchy-Schwartz si ha anche: $$\frac{\mid (d_{k},\nabla f(x_{k}))\mid}{\parallel d_{k}\parallel}\leq \frac{\parallel d_{k}\quad \parallel \parallel \nabla f(x_{k})\parallel}{\parallel d_{k}\parallel}=\parallel \nabla f(x_{k})\parallel$$ che mostra come la direzione di ricerca sia quella in cui la derivata direzionale di $f$ \`e negativa e di modulo massimo.\\
Le condizioni di arresto del metodo sono: $\parallel x_{k+1}-x{k}\parallel \leq m$, $\parallel \nabla f(x_{k+1})\parallel \leq m\prime$ oppure $k>k_{max}$, dove $m$ e $m\prime$ sono soglie date e $k_{max}$ il numero massimo di iterazioni da effettuare.\\
I risultati ottenuti sono garantiti dai seguenti teoremi di convergenza:
\newtheorem{teo}{Teorema}
\begin{teo}
Sia $f(x)\in C^{1}$, strettamente convessa sull'insieme $\Sigma_{0}=\{x\in\mathbb{R}^{n}:f(x)\leq f(x_{0})\}$, e la successione $\{x_{k}\}$ sia generata tramite l'algoritmo (\ref{eq:min}). Si supponga
\begin{enumerate}
\item che l'insieme $\Sigma_{0}$ sia compatto;
\item che le direzioni $d_{k}$ siano t.c. $\frac{(d_{k},\nabla f(x_{k}))}{\parallel d_{k}\quad \parallel \parallel \nabla f(x_{k})\parallel}\leq - \cos \theta$ per $k\in \textit{I}$, con $\textit{I}$ insieme illimitato di indici;
\item che per $k\in \textit{I}$, $\beta_{k}$ sia ottenuto tramite ricerca esatta.
\end{enumerate}
Allora la successione $\{x_{k}\}$ converge all'unico punto $x^{*}$ di minimo per $f$.
\end{teo}
\begin{teo}
Sia $f(x)\in C^{2}$, strettamente convessa sull'insieme (che si suppone compatto) $\Sigma_{0}=\{x\in\mathbb{R}^{n}:f(x)\leq f(x_{0})\}$, e la successione $\{x_{k}\}$ sia generata tramite l'algoritmo (\ref{eq:min}), con $d_{k}=-\nabla f(x_{k})$.\\
Allora, se i passi $\beta_{k}$ sono determinati tramite ricerca esatta, la successione $x_{k}$ converge all'unico punto $x^{*}$ di minimo per f.
\end{teo}
Minimizzare l'errore di allenamento non necessariamente comporta l'ottimizzazione di apprendimento dell'algoritmo, potrebbe verificarsi il fenomeno di adattamento insufficiente (\textit{underfitting}),ovvero non si hanno abbastanza dati per creare un modello di predizione accurato . Bisogna quindi valutare anche altri fattori: analizzare l'insieme di prova.\\
Ricordandoci dell'eq.\ref{train-error}, calcoliamo l'errore di prova:
\begin{equation}\label{test-error}
L(y,\hat{y})_{test}=\frac{1}{n}\sum_{i=1}^{n}(\hat{y}_{(test)}-y_{(test)})_{i}^2
\end{equation}
Vorremmo che ,con i parametri trovati per minimizzare l'errore di allenamento, anche questo errore sia minimo (l'ottimalità \`e 0). Ma come detto in precedenza non sempre questo accade, vorremmo quindi che il divario tra i due errori sia minimo. In caso contrario si verifica il fenomeno di adattamento eccessivo (\textit{overfitting}) del modello all'insieme di dati che descrive, tramite un eccessivo numero di parametri. Il modello quindi non sarà generalizzabile ad un nuovo insieme di dati.\\
Consideriamo il valore atteso dell'errore di prova, calcolato prendendo una coppia di punti $(X,Y)$dall'insieme di prova:
\begin{equation}\label{test}
\mathbb{E} [L(y,\hat{y})_{test}]=\mathbb{E} [(Y-\hat{y}(X))^{2}]
\end{equation}
e definiamo la funzione dell'output effettivo come:
$$y(X)=\mathbb{E}(Y|X)$$
la quale avr\'{a} sicuramente un errore, dovuto da qualche interferenza che chiameremo: distorzione stimata (\textit{estimation bias}).\\
Ma con diversi insiemi di allenamento, possiamo costruire diverse funzioni $\hat{y}$, e anche questo \`e un'altra fonte di errore: la varianza stimata (\textit{estimation variance}). Possiamo quindi scrivere l'output come:
$$Y=y(X)+\epsilon$$ con $\epsilon$ indipendente da $X$ tale che $\mathbb{E}[X]=0$ e $Var(X)=\sigma^{2}$.\\
Possiamo quindi riscrivere l'equazione \ref{test} come:
\begin{eqnarray}
\mathbb{E} [L(y,\hat{y})_{test}] &=&\mathbb{E} [(Y-\hat{y}(X))^{2}|X=x]\nonumber\\
&=&\mathbb{E}[(Y-y(x))^2 |X=x]+\mathbb{E}[(y(x)-\hat{y}(x))^{2}|X=x]\nonumber\\
&=&\sigma^{2}+\mathbb{E}[(y(x)-\hat{y}(x))^{2}]
\end{eqnarray}
dove $\sigma^{2}$ \`e chiamato errore Bayes e
\begin{eqnarray}
\mathbb{E}[(y(x)-\hat{y}(x))^{2}]&=&(\mathbb{E}[\hat{y}(x)]-y(x))^{2}+\mathbb{E}[(\hat{y}(x)-\mathbb{E}[\hat{y}(x)])^{2}]\nonumber\\
&=&Bias(\hat{y}(x))^{2}+Var(\hat{y}(x))
\end{eqnarray}
Si ottiene così il compromesso distorzione-varianza (\textit{bias-variance tradeoff}):
\begin{equation}
\mathbb{E} [L(y,\hat{y})_{test}]=\sigma^2+Bias(\hat{y}(x))^2 + Var(\hat{y}(x))
\end{equation}
Se la distorzione ha valori alti e la varianza bassi avremo un fenomeno di adattamento insufficiente, mentre se la distorzione ha valori bassi e la varianza alti avremo un adatteamento eccessivo.\cite{errval}
Un modo per equilibrare questo compromesso ï¿½ usare la convalida incrociata (\textit{Cross-Validation}), che consiste nel ripetere l'addestramento e il test dell'algoritmo ogni volta su sottoinsiemi scelti in maniera casuale.\\
Alcune varianti di convalida incrociata verranno analizzate in seguito.

\section{Apprendimento supervisionato}
Gli algoritmi di apprendimento supervisionato vengono utilizzati per risolvere problemi di classificazione e di regressione, che analizzeremo rispettivamente nel cap. \ref{classificazione} e nel paragrafo \ref{regressione}.
Si parla di apprendimento supervisionato quando il dataset che si utilizza contiene delle variabili, una delle quali \`e un'etichetta. Dato un vettore di input $x=(x_{1},\cdots,x_{n})$, ogni $x_{i}$ \`e un vettore d-dimensionale di numeri rappresentanti una caratteristica, da questi dati si costruisce l'insieme di addestramento di cardinalità N: $D={\{ (x_{i},y_{i})\}}^{N}_{i=1}$, dove $y=(y_{1},\cdots,y_{m})$ \`e l'output dei risultati desiderati e $y_{i}$ \`e l'etichetta.
Lo scopo \`e di apprendere una regola generale che colleghi i dati in ingresso con quelli in uscita, in modo che l'algoritmo apprenda a classificare un esempio completamente nuovo, non contenente l'etichetta.\\
Se $y_{i}$ \`e di tipo testuale si parla di classificazione, quando invece \`e di tipo numerico si parla di regressione. Se indichiamo con C il numero delle classi a cui può appartenere l'output: $y\in \{1,...,C\}$, se $C=2$ la classificazione sarà binaria (in questo caso spesso $y\in \{0,1\}$); se $C>2$ sarà multiclasse.\\

\subsection{Regressione}\label{regressione}
La Regressione prevede il valore futuro di un dato, avendo noto il suo valore attuale. Un esempio \`e la previsione della quotazione delle valute o delle azioni di una società. Nel marketing viene utilizzato per prevedere il tasso di risposta di una campagna sulla base di un dato profilo di clienti; nell'ambito commerciale per stimare come varia il fatturato dell'azienda al mutare della strategia. Questo avviene costruendo una funzione che meglio si adatta ai
punti che descrivono la distribuzione delle Y sulle X. 
\subsubsection{Regressione lineare}
Preso un vettore $x\in \mathbb{R}^{n}$ in input, l'algoritmo cerca di prevedere l'output: $y\in \mathbb{R}$. Dove $y=f(x)$, con $f$ una funzione lineare:
\begin{flushright}
$y=\alpha+\beta x \qquad \qquad (retta\;di\;regressione)$
\end{flushright}
Sia $\hat{y}$ il valore di output che l'algoritmo prevede. Definiamo l'output come:
$$\hat{y}=\alpha+\beta x+\epsilon$$
dove $\epsilon$ \`e la componente di errore.\\
Ora per identificare la retta che meglio si adatta ai punti, che descrivono la distribuzione delle $y$ sulle $x$, bisogna stimare i valori dei parametri $\alpha$ e $\beta$, tramite i dati ossrvati su un esempio.\\
Usiamo il metodo dei minimi quadrati (\textit{least squares}), che minimizza l'errore $\epsilon$.
\begin{eqnarray}
\sum_{i=1}^{n}(\hat{y}_{i}-y_{i})^{2}&=&\sum_{i=1}^{n}(\hat{y}_{i}-(\alpha+\beta x_{i}))^{2}\nonumber \\
&=&\sum_{i=1}^{n}(\hat{y}_{i}-\alpha-\beta x_{i})^{2}=\min \nonumber
\end{eqnarray}
Per trovare i valori di $\alpha$ e $\beta$ risolviamo il sistema:
\begin{empheq}[left=\empheqlbrace]{align}
\frac{d (\, \sum_{i=1}^{n}(\hat{y}_{i}- \alpha - \beta x_{i})^{2})}{d \, \alpha} = 0 \label{a} \\
\frac{d (\, \sum_{i=1}^{n}(\hat{y}_{i}- \alpha - \beta x_{i})^{2})}{d \, \beta} = 0 \label{b}
\end{empheq}
Dalla (\ref{a}) otteniamo:
$$\alpha=\mathbb{E}[y]-\beta \mathbb{E}[x]$$
e dalla (\ref{b}):
$$\beta=\frac{\sum_{i=1}^{n}(x_{i}-\mathbb{E}[x])(y_{i}-\mathbb{E}[y])}{\sum_{i=1}^{n}(x_{i}-\mathbb{E}[x])^2}=\frac{Cov(x,y)}{Var(x)}$$
ottenendo così i valori dei parametri che cercavamo.\\


\section{Apprendimento non supervisionato}
Gli algoritmi di apprendimento non supervisionato vengono utilizzati per risolvere problemi di raggruppamento.
All'algoritmo viene passato solo l'input: $D={\{ x_{i}\}}^{N}_{i=1}$ e cerca una relazione tra i dati per capire se e come essi siano collegati tra di loro. Non contenendo alcuna informazione preimpostata, l'algoritmo \`e chiamato a creare una "nuova conoscenza" (\textit{knowledge discovery}). A differenza del caso supervisionato, questo apprendimento non ha una classificazione o un risultato finale con il quale determinare se il risultato \`e attendibile, ma generalizza le caratteristiche dei dati e in base ad esse attribuisce ad un input un output: serve generalmente ad estrarre informazioni non ancora note, "creando" esso stesso delle classi in cui dividere i dati. Questa tecnica si chiama \textit{clustering}.

\section{Apprendimento per rinforzo}
Gli algoritmi di apprendimento per rinforzo vengono utilizzati per risolvere problemi di regressione.
Lo scopo di questo algoritmo \`e di realizzare un sistema in grado di apprendere ed adattarsi ai cambiamenti dell'ambiente in cui si trovano, attraverso la distribuzione di una "ricompensa" detta rinforzo, data dalla valutazione delle prestazioni. Questi algoritmi sono costruiti sull'idea che i risultati corretti dovrebbero essere ricordati, per mezzo di un segnale di rinforzo, in modo che diventino più probabili e quindi più facilmente riottenuti nelle volte future; viceversa se il rusultato \`e errato , il segnale sarà una penalità, ovvero si avrà una probabilità più bassa legata a quel determinato output.\cite{rinforzo}

\chapter{Reti neurali artificiali}
Le reti neurali sono i modelli di deep learning per eccellenza. Sono un sistema di elaborazione di informazioni ispirato al funzionamento del sistema nervoso umano.\\
La rete \`e strutturata come un grafo orientato. I nodi sono raggruppati in strati (\textit{layers}): il primo strato contiene i nodi di input  $x_1,\cdots,x_n$, connessi con lo strato successivo, dove ad ogni arco \`e associato un peso $w_i$. L'ultimo strato contiene i nodi di output. Gli strati tra il primo e l'ultimo strato sono chiamati strati nasconti (\textit{hidden layers}). La lunghezza complessiva del percorso determina la profondità del modello, da cui deriva il nome dell'apprendimento: "deep learning".\\
In base all'architettura scelta, esistono vari modelli di  reti neurali;
la scelta dell'architettura della rete \`e molto importante, poiché in base al numero di nodi usati per ogni strato ed alla profondità, il costo computazionale cresce o diminuisce: per esempio la scelta di un'architettura poco profonda e con una elevata quantità di nodi per strato, causa un costo computazionale elevato ed un massiccio utilizzo della memoria.\\
Lo scopo di questa tesi \`e di trovare l'architettura ottimale per un algoritmo di classificazione di immagini considerando anche le geolocalizzazione dell'oggetto da riconoscere (e del suo visualizzatore). \\ %vedere se toglierlo 
Ritornando alle reti neurali, in generale, abbiamo detto che si ispirano al nostro sistema nervoso; vediamo brevemente in che modo:\\
Ogni neurone, nel nostro cervello, riceve un'intera serie di segnali da altri neuroni, li somma all'interno del suo corpo cellulare e, sulla base della somma, aggiusta la frequenza delle scariche da inviare alla
cellula successiva. I neuroni ricevono sia segnali eccitatori, ovvero che tendono ad aumentare la loro frequenza di scarica, che segnali inibitori, che tendono invece a diminuirla, ma nonostante ricevano due tipi di segnali, ne emettono poi di un solo tipo. Analogamente: ogni neurone aritificiale, rappresentato da un nodo, diventa attivo se la quantità totale di segnale che riceve supera la soglia di attivazione, definita dalla cosiddetta funzione di attivazione. Se un nodo diventa attivo, emette un segnale che viene trasmesso lungo i canali di trasmissione fino all'altra unità a cui \`e collegato.\\

\begin{figure}[ht!]
\centering
\includegraphics{reteneurale}
\caption{modello non lineare di un neurone artificiale}\label{neurone}
\end{figure}
La figura \ref{neurone}, indipendentemente dal modello di rete utilizzato, mostra l'elaborazione eseguita da un neurone artificiale.\\
Siano $x_1,\cdots,x_n$ i dati in input, rappresentati dagli $n$ nodi del primo strato, nel k-esimo neurone l'informazione viene elaborata come:
$$y_k=f(b_k+\sum_{i=1}^n w_{ki}\cdot x_i)$$
dove:\\
\begin{itemize}
\item $y_k$ \`e l'output generato dal neurone k;
\item $b_k$ \`e il valore soglia del neurone k;
\item $w_{ki}$ \`e il peso associato all'arco che collega il nodo i-esimo al neurone $k$;
\item $f(\cdot)$ \`e la funzione di attivazione.
\end{itemize}
Il motivo principale per cui vengono scelte le reti neurali \`e la possibilità di parallelizzare i calcoli.
\section{Funzioni di attivazione}
La funzione di attivazione \`e una funzione usata per normalizzare, quindi limitare, l'ampiezza dell' output, così da non consumare eccessiva memoria e di velocizzare il processo di calcolo .\\
Generalmente le reti neurali sono utilizzate per implementare funzioni complesse e le funzioni di attivazione non lineari consentono loro di approssimare funzioni arbitrariamente complesse. Le funzioni più utilizzate a tale scopo sono 3:
\begin{figure}[ht!]
\includegraphics[scale=0.7]{funzioniAttivazione.png}
\caption{Grafici delle funzioni di attivazione più usate.}
\end{figure}
Vediamo nel dettaglio quali sono.
\subsubsection{Sigmoid}
La funzione Sigmoide viene usata sopartutto nei modelli in cui si deve prevedere la probabilità come output, poichè il suo codominio \`e l'intervallo: $(0,1)$ ed ed \`e definita come: $S:\mathbb{R}\longrightarrow(0,1)$ , tale che:
$$S(x)=\frac{1}{1+\exp^{-x}}$$
La Sigmoide \`e una funzione monotona e differenziabile, infatti la sua derivata:
$$\frac{dS}{dx}=\frac{\exp^{-x}}{(1+\exp^{-x})^2}$$
\`e continua, positiva e derivabile in  ogni punto del dominio.
\subsubsection{tanh}
La tangente iperbolica: $\tanh:\mathbb{R}\longrightarrow(-1,1)$ ,
$$
\tanh(x)=\frac{2}{1+\exp^{-2x}} - 1
$$ 
\`e simile alla Sigmoide, infatti sono legate dalla relazione:
$$
\tanh(x)=2 \cdot S(2x)-1
$$
ma ha un intervallo di output più ampio. Questo le consente di produrre anche output di segno negativo.\\
Anche la tangente iperbolica \`e una funzione monotona e differenziabile.\\
Purtroppo essendo funzioni limitate, per valori alti tendono ad un valore specifico: $\lim_{x \to +\infty} \tanh(x)=1$, questo comportamento può portare ad una perdita di informazioni: se $x$ corrisoponde ad un valore elevato e subisce una grande variazione, per la funzione invece avrà avuto una variazione minima. Lo stesso problema si presenta con le derivate, provocando la "scomparsa del gradiente"(\textit{vanishing gradient}), fondamentale per approssimare al meglio la funzione; ma questo argomento verrà affrontato in \ref{back-prop}.
\subsubsection{ReLu}
La funzione ReLu (Rectified Linear Unit) \`e la funzione pi\'{u} utilizzata nel deep learning e, in particolare, nelle reti convoluzionali. \`{E} definita:
$$Relu:\mathbb{R}\longrightarrow [0,\infty)$$
$$
Relu(x)=
\left\{
\begin{array}{rl}
x & \mbox{se } x \geq 0 \\
0 & \mbox{se } x < 0
\end{array}
\right.
$$
Sia la funzione, che la sua derivata sono monotone. A differenza delle funzioni precedenti, la sua derivata \`e molto semplice da calcolare:
$$
Relu(x)^{\prime}=
\left\{
\begin{array}{rl}
1 & \mbox{se } x \geq 0 \\
0 & \mbox{se } x < 0
\end{array}
\right.
$$
e non rischia di incorrere nella sparizione del gradiente. Si potrebbero avere problemi nell'origine, per la presenza di un punto angoloso, poich\'{e} la derivata è indefinita ma, per convenzione, viene definita uguale a zero.\\
Risulta immediato il motivo per cui sono molto utilizzate nelle reti formate da tanti strati: mappando i valori negativi in zero, permette di tenere solo i valori positivi, riducendo di molto il numero di neuroni attivati. \\
\section{Addestramento di una rete}
Come visto in \ref{Costruzione}, per addestrare un algoritmo si ha bisogno di una funzione di perdita e di un metodo per minimizzare l'errore di valutazione. L'addestramento di una rete neurale si basa sugli stessi principi:\\
si definisce una mappa $$y=f(x,\theta)$$ e si cerca il valore del parametro $\theta$ che più accuratamente approssima la funzione.
Bisogna quindi trovare dei parametri che minimizzano la funzione di perdita:
$$L[y,f(x,\theta)]$$
ovvero, trovare $\hat{\theta}$ tali che:
$$\hat{\theta}=\argmin_{\theta} \left\{ \frac{1}{n} \sum_{i=1}^{n} L[y_i,f(x_i,\theta)] \right\}$$
Questo processo è necessario per approssimare una determinata funzione $f^*$, che descrive il comportamento della rete.\\
Durante l'addestramento, la rete viene provata pi\'{u} volte, ogni volta con un inpur diverso, fino a che l'errore di addestramento \`e molto piccolo. In questa fase, ad ogni iterazione viene passato in input un insieme di addestramento, viene fissato un valore $\theta_{0}$ iniziale, e tramite la funzione di perdita, si calcola l'errore di addestramento. In questo modo la rete ha il valore di quanto ciascun neurone di output sia lontano dal proprio valore atteso, e in che direzione (positiva o negativa).Per esempio, nel caso di riconoscimento dei numeri scritti a mano (MNIST):\\
Se il risultato atteso è 6, ci aspettiamo il valore 1 nel neurone 6 e 0 in tutti gli altri. Se al neurone 6 \`e associato il valore $0.7$, allora la correzione da fare  è di 0.3. Quindi la rete ripete l'addestramento aggiornando il peso corrispondente al neurone valutato con $\theta_{1}$ e cos\'{i} via, fino a che il valore corrispondente al neurone 6 \`e molto vicino a 1. Viceversa, se il neurone 4 invece di 0 presenta 0.8, allora la correzione sarà -0.8 e si aggiorneranno i pesi relativi a questo neurone in modo da abbassarne drasticamente l'output.\\
L'algoritmo appena descritto a parole prende il nome di \textit{back propagation}.
\subsubsection{Algoritmo di back propagation}\label{back-prop}
Consideriamo una rete neuronale composta da L strati, dove l'output \`e composto da un valore solamente.
L'algoritmo di back propagation si divide in due fasi:
\begin{itemize}
\item propagazione in avanti
\item propagazione all'indietro
\end{itemize}
Durante la propagazione in avanti, si calcolano tutti i valori dei nodi presenti nella rete $a_{i}^k$:
\begin{equation}\label{eq0}
a_{i}^{k}=b_{i}^{k}+\sum_{j=1}^{r_{k-1}} w_{ji}^{k}\cdot o_{j}^{k-1}
\end{equation}
dove:
\begin{itemize}
\item $w_{ij}^{k}$ \`e il peso associato all'arco che collega il nodo i del k-esimo strato con il nodo j;
\item $b_{i}^{k}$ \`e il valore soglia del nodo i nel k-esimo strato;
\item $o_{i}^{k}$ \`e l'output del nodo i nel k-esimo strato;
\item $r_k$ \`e il numero di nodi presenti nel k-esimo strato.
\end{itemize}
Calcolato l'ultimo valore $a^{L}$, corrispondente all'output della rete, inizia la seconda fase. Per calcolare il gradiente della funzione di perdita $L[y,f(x,\theta)]$, si applica la regola della catena per il calcolo e poich\'{e} $\theta = (W^1,W^2,\cdots ,W^{L-1})$, abbiamo:
\begin{equation}\label{eq1}
\frac{\partial L}{\partial w_{ij}^k}=\frac{\partial L}{\partial a_j^k}\frac{\partial a_j^k}{\partial w_{ij}^k}.
\end{equation}
Ora definiamo il punto di forza di questo algoritmo, che consiste nell'introdurre una quantit\'{a} di costo (anche chiamata errore):
\begin{equation}\label{eq2}
\delta_j^k\equiv \frac{\partial L}{\partial a_j^k}
\end{equation}
attraverso la quale si calcolano le derivate parziali in modo iterativo, ripercorrendo la rete all'indietro.\\
Dall'eq \ref{eq0}, ricaviamo:
\begin{equation}
\frac{\partial a_j^k}{\partial w_{ij}^k}=\frac{\partial}{\partial w_{ij}^k}\left( \sum_{l=0}^{r_{k-1}}w_{lj}^k o_l^{k-1}\right)=o_i^{k-1}
\end{equation}
che sostituita nell'eq \ref{eq1} otteniamo:
\begin{equation}
\frac{\partial L}{\partial w_{ij}^k}=\delta_j^k o_i^{k-1}
\end{equation}
Partendo dallo strato dell'output, considerando come funzione di perdita: $$L=\frac{1}{2}(\hat{y}-y)^2=\frac{1}{2}(g_o(a^L)-y)^2,$$
\begin{equation}
\delta^{L}=(g_o(a^L)-y)g_o^{\prime}(a^L)=(\hat{y}-y)g_o^{\prime}(a^L).
\end{equation}
dove $g_o$ \`e la funzione di perdita per l'output.
Quindi la derivata parziale della funzione di perdita \`e:
\begin{equation}
\frac{\partial L}{\partial w_{iL-1}^L}=\delta^L o_i^{L-1}
\end{equation}
Per gli strati nascosti invece abbiamo:
\begin{equation}\label{eq3}
\delta_j^k=\sum_{l=1}^{r_{k+1}}\frac{\partial L}{\partial a_l^{k+1}}\frac{\partial a_l^{k+1}}{a_j^k}=\sum_{l=1}^{r_{k+1}} \delta_l^{k+1} \frac{\partial a_l^{k+1}}{a_j^k},
\end{equation}
Sostituendo in \ref{eq3} la \ref{eq0} e denotando con $g$ la funzione di perdita dello strato nascosto, si ottiene la formula di propagazione all'indietro:
\begin{equation}
\delta_j^k=g^{\prime}(a_j^k)\sum_{l=1}^{r_{k+1}}w_{jl}^{k+1}\delta_l{k+1}
\end{equation} 
e la derivata parziale della funzione di perdita rispetto ai pesi degli strati nascosti $w_{ij}^{k+1}$ , $1\leq k < L$, si ottiene con:
\begin{equation}
\frac{\partial L}{\partial w_{ij}^k}=\delta_j^k o_i^{k-1}=g^{\prime}(a_j^k)o_i^{k-1}\sum_{l=1}^{r_{k+1}}w_{jl}^{k+1}\delta_l{k+1}
\end{equation}
Ora pu\'{o} avvenire l'aggiornamento dei pesi tramite il metodo SGD (Stochastic Gradient Descent), ovvero in formule:
\begin{equation}\label{eq4}
w_{ij}^{k+1}=w_{ij}^k + \eta \frac{\partial L}{\partial w_{ij}}
\end{equation}
dove $\eta \in (0,1]$ rappresenta il fattore di apprendimento (\textit{learning rate}).\\
La scelta del fattore di apprendimento influenza molto il comportamento dell'algoritmo, infatti se scegliamo valori troppo piccoli, la convergenza sarà lenta, mentre se scegliamo valori troppo grandi si rischia di avere una rete instabile con comportamento oscillatorio.\\
Un metodo semplice per incrementare il fattore di apprendimento, senza il rischio di rendere la rete instabile, è quello di modificare la regola di aggiornamento inserendo un ulteriore parametro $\alpha$, detto momento nell'equazione \ref{eq4}:
\begin{equation}
w_{ij}^{k+1}=\alpha w_{ij}^k + \eta \frac{\partial C}{\partial w_{ij}}.
\end{equation}
Se si espande ricorsivamente la formula si ottiene:
\begin{equation}
w_{ij}^{k+1}=\eta \sum_{l=1}{k+1}\alpha_{k+1-l}\frac{\partial L}{\partial w_{ij}^l}
\end{equation}
Inserendo il momento si ha il vantaggio che se la derivata parziale tende a mantenere lo stesso segno su iterazioni consecutive, grazie alla sommatoria, l'aggiornamento produce valori pi\'{u} ampi e quindi tende ad accelerare nelle discese del gradiente. Se invece la derivata ha segni opposti ad iterazioni consecutive, la sommatoria tende a diminuire l'ampiezza dell'aggiornamento, in questo modo non si corre il rischio di avere dei loop infiniti nel caso di minimi locali della funzione d'errore.\\
A tal proposito generalmente i criteri di arresto dell'algoritmo possono essere:
\begin{itemize}
\item $\lVert \nabla L\rVert < \epsilon$
\item $L[y,f(x,\theta)]=0$;
\item $L_i[y,f(x,\theta_i)]-L_j[y,f(x,\theta_j)]<<\epsilon^{\prime}$
\end{itemize}
dove $L_i$ e $L_j$ sono due epoche consecutive. Un'epoca corrisponde alle due fasi di propagazione necessarie per un aggiornamento dei pesi.

\section{Reti Deep Feed-forward}
Le reti Feed-forward sono le reti neurali profonde con la struttura più semplice, composte da almeno uno strato nascosto. La loro struttura è rappresentata da un grafo aciclico diretto in un'unica direzione, dove ogni nodo di uno strato \`e connesso con tutti i nodi dello strato successivo e nessun nodo è connesso con un nodo appartanente allo stesso strato.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{network-neurale}
\caption{modello di rete deep feedforward con uno strato}\label{rete}
\end{figure}
\section{Reti Convoluzionali}





\section{ImageNet}
In Internet oramai sono presenti milioni di milioni di immagini e di video, usati per i modelli e gli algoritmi piï¿½ sofisticati e robusti, per aiutare gli utenti ad indicizzare, recuperare, organizzare e interagire con questi dati.
Un esempio banale ï¿½ la ricerca dell'immagine di un cane su Google: se noi volessimo ricercare una razza canina in particolare, come fa l'algoritmo a farci visualizzare un cane di razza Labrador piuttosto che Beagle? Tramite categorie e sottocategorie. Ma esattamente come tali dati possono essere utilizzati e organizzati ï¿½ un problema che non ï¿½ ancora stato risolto.\\
Di solito i motori di ricerca possono trovare una determinata immagine solo se il testo inserito corrisponde al testo con cui ï¿½ stato etichettato. Ma le etichette possono essere inaffidabili, inutili (nel caso di dialetti e nomignoli) o semplicemente inesistenti. Si vuole quindi cercare un metodo per far imparare alle macchine come riconoscere oggetti simili senza etichetta, rendendo possibile un notevole aumento dell'accuratezza del riconoscimento.\\
Si ï¿½ creato cosï¿½ \textbf{ImageNet}: un grande database visivo contenente oltre 14 milioni di immagini etichettate, progettato per l'utilizzo nella ricerca di software per il riconoscimento di oggetti visivi.\\
ImageNet si basa sulla struttura gerarchica fornita da un altro database: \textit{WordNet}. WordNet ï¿½ un database semantico-lessicale per la lingua inglese elaborato dal linguista George Armitage Miller presso l'Universitï¿½ di Princeton, che si propone di organizzare, definire e descrivere i concetti espressi dai vocaboli. L'organizzazione del lessico si avvale di raggruppamenti di termini con significato affine, chiamati \textit{"synset"} (dalla contrazione di synonym set), e del collegamento dei loro significati attraverso diversi tipi di relazioni chiaramente definite. All'interno dei synset le differenze di significato sono numerate e definite.\cite{wordnet}\\
%chiedere se devo specificare che questa ï¿½ la grafica bitmap%
Un'immagine digitale ï¿½ formata da diversi quadratini disposti in modo regolare, su una griglia di punti equidistanti, questi quadratini sono detti \textit{pixel (picture elements)}. I pixel presenti in um'immagine ne determinano la dimensione e la risoluzione, per esempio un'immagine di dimensione $320 x 240$ pixel indica che sono presenti 240 pixel orizzontali e 320 verticali. Piï¿½ i pixel sono numerosi, e quindi piï¿½ piccoli e fitti, piï¿½ la risoluzione ï¿½ alta. In ogni pixel risiede un'informazione espressa in bit riguardante (generalmente) il colore: un pixel da 1 bit puï¿½ avere solo $2^1$colori, mentre uno da 1 byte (8bit)  puï¿½ rappresentare $2^8=256$ colori. Il numero di colori possibili ï¿½ detto anche profonditï¿½. Le rappresentazioni delle immagini a colori variano a seconda dei campi di colore che si usano, solitamente, ogni campo viene rappresentato da 1 byte e rappresenta il livello di intensitï¿½ dei colori fondamentali. Il modello piï¿½ utilizzato ï¿½ quello RGB, dove per ogni pixel vengono utilizzati 3byte:
\begin{itemize}
\item 1 byte per la componente rossa (R)
\item 1 byte per la componente verde (G)
\item 1 byte per la componente blu (B)
\end{itemize}
ma ne esistono anche altri come, ad esempio, CMYK che considera come colori fondamentali: ciano, magenta, giallo e nero.\\
Tipicamente nel data set di ImageNet la dimensione media delle immagini \`e di circa $400x350$







\chapter{Classificazione}\label{classificazione}
La Classificazione viene usata quando \`e necessario decidere a quale categoria appartiene un determinato dato.Per esempio, data una foto capire a quale categoria appartiene, in questa tesi vogliamo classificare immagini, pi\'{u} precisamente: capire a quale tipo di monumento corrispone una determinata immagine.\\ 
Questo tipo di algoritmo deve specificare a quale delle k categorie appartiene un input. Crea una funzione $f:\mathbb{R}^{n} \rightarrow \{ 1,\cdots,k\}$, quando $y=f(x)$, il modello assegna l'input descritto dal vettore $x$ ad una categoria identificata dal codice numerico $y$. Esistono altre varianti dell'attivitï¿½ di classificazione, ad esempio, dove $f$ genera una distribuzione di probabilitï¿½ su classi.\\
Vediamo ora qualcuno degli algoritmi usati per classificare dati.

\section{K-Nearest Neighbor}
Questo ï¿½ un algoritmo non parametrico, ovvero il numero di parametri cresce con la quantitï¿½ di dati di addestramento.
Dato un set di dati di addestramento $(x_1,x_2,\cdots,x_n)$, corrispondenti ai risultati $(y_1,y_2,\cdots,y_n)$, questo metodo ,dato un nuovo punto $z$, cerca di prevedere la sua classe di appartenenza osservando, tra l'insieme di punti adiacenti, quelli a lui piï¿½ vicini. Il numero di punti adiacenti da considerare dipende dal parametro $k$: si osservano le classi a cui appartengono i $k$ punti piï¿½ vicini e la classe piï¿½ ricorrente sarï¿½ assegnata al punto $z$.\\
Vediamo ora come affrontare il problema:\\
Supponiamo di avere un set di dati che comprende $N_k$ punti appartenenti alla classe $C_k$ con $N$ punti in totale, ovvero: $\sum_{k}N_k = N$. Se vogliamo classificare un nuovo punto $x$, disegniamo una sfera centrata su $x$ contenente  K punti qualsiasi, indipendentemente da come siano classificati. Vogliamo ora calcolare la funzione di probabilitï¿½ di $x$ che (essendo in un caso di classificazione i valori sono discreti) sarï¿½ la densitï¿½ discreta:
\begin{equation}\label{dens}
P=\int_R p(x) dx
\end{equation}
Supponiamo ora di aver raccolto un set di dati comprendente N osservazioni con probabilitï¿½ uniforme $p(x)$ di essere all'interno della regione R, quindi la probabiltï¿½ di avere K punti all'interno di R sarï¿½ data dalla distribuzione binomiale:
\begin{equation}\label{binomiale}
P(X=K)=\frac{N!}{K!(N-K)!}P^K (1-P)^N-K,
\end{equation} 
dove il valore atteso e la varianza sono date da:
\begin{equation}
\mathbb{E}[K]=NP \qquad \qquad Var[K]=NP(1-P)
\end{equation}
Per N grandi, applichiamo il Teorema di De Moivre-Laplace alla eq.\ref{binomiale} e otteniamo che la distribuzione binomiale si comporta come una distribuzione normale con stessa media e varianza della binomiale, perciï¿½ possiamo assumere: \begin{equation} \label{eq1}
K\simeq NP
\end{equation}
Ora se assumiamo che la regione R sia sufficientemente piccola e che la densitï¿½ di probabilitï¿½ $p(x)$ sia approssimativamente costante in R, da \ref{dens} otteniamo:
\begin{equation} \label{eq2}
P\simeq p(x)V
\end{equation}
dove V ï¿½ il volume della sfera R. Combinando le eq. \ref{eq1},\ref{eq2} otteniamo:
\begin{equation}
p(x)=\frac{K}{NV}
\end{equation}
che fornisce le seguenti stime:
\begin{eqnarray}
p(x\textbar C_k)=\frac{K_k}{N_k V}\\
p(x)=\frac{K}{NV}\\
p(C_k)=\frac{N_k}{N}
\end{eqnarray}
che sostituite nel Teorema di Bayes otteniamo:%segnarsi da qualche parte la dimostrazione
\begin{equation}
p(C_k\textbar x)=\frac{p(x\textbar C_k)p(C_k)}{p(x)}=\frac{K_k}{K}
\end{equation} 
Quindi per minimizzare la probabilitï¿½ di errore di classificazione, bisogna assegnare al nuovo punto x la classe con probabilitï¿½ piï¿½ alta ovvero quando il valore $\frac{K_k}{K}$ ï¿½ massimo.\\
L'obiettivo del metodo ï¿½ quindi chiaro: per classificare un nuovo punto, identifichiamo i K punti piï¿½ vicini all'insieme di allenamento e quindi assegniamo il nuovo punto alla classe che ha il maggior numero di rappresentanti tra i K punti. Rimane solo la scelta della metrica da usare per calcolare la distanza tra i punti, solitamente viene usata la distanza Euclidea, ma questo dipende dal problema e dalla tipologia del dato da analizzare.
Lo svantaggio di questo algortimo ï¿½ chiaro: il numero di distanze da calcolare aumenta con l'aumentare dell'insieme di addestramento. Oltre a rallentare il tempo di calcolo, si usa anche una considerevole quantitï¿½ di memoria, per ovviare a questo si divide l'insieme di allenamento in sottoinsiemi di cardinalitï¿½ n, dove n di solito ï¿½ un divisore della cardinalitï¿½ dell'insieme totale e prende il nome di \textit{batch size}. Diminuendo il numero di dati per l'addestramento i calcoli e la memoria necessari diminuiscono notevolmente, in quanto i paragoni e i dati significativi da mantenere sono minori.%inserire conlcusione vantaggi e svantaggi fatta bene e foto che chisrisce il metodo 

\section{Alberi di decisione}

\chapter{TensorFlow}
TensorFlow (TF) ï¿½ una libreria software open source, sviluppata da Google, utilizzata per implementare l'apprendimento automatico e i sistemi di deep learning.\\
TensorFlow fornisce API native in linguaggio: Python, C/C++, Java, Go, e RUST. Noi useremo il linguaggio di programmazione Python.\\
In generale un algoritmo scritto in TF rispetta la seguente struttura:
\begin{enumerate}
\item Importare ed analizzare l'insieme di dati
\item Creare colonne di caratteristiche per descrivere i dati
\item Selezionare il tipo di modello
\item Provare il modello
\item Valutare l'efficacia del modello
\item Lasciare che il modello addestrato faccia previsioni (test)
\end{enumerate}
che ï¿½ in linea con la descrizione di un generico algoritmo di apprendimento descritto in (\ref{Costruzione}). La vera innovazione di TF ï¿½ come descrive il modello, poichï¿½ lo fa costruendo un grafico computazionale: \textit{Data Flow Graph}. In questo grafico ogni nodo rappresenta l'istanza di un'operazione matematica, mentre ogni spigolo rappresenta un tensore, su cui vengono eseguite le operazioni.
\begin{defin}
Un \textbf{tensore} in TF  ï¿½ una matrice n-dimensionale di tipi di dati di base (es: float32, int32,string, ecc..). Viene chiamato tf.Tensor ed ï¿½ descritto da tre parametri:\begin{enumerate}
\item grado  (\textit{rank})
\item corpo (\textit{shape})
\item tipo (\textit{type})
\end{enumerate}
\end{defin}
\textbf{1)} Il grado di un oggetto tf.Tensor ï¿½ il suo numero di dimensioni. Come mostra la seguente figura %inserire figura dei tensori
, ogni rank in TensorFlow corrisponde a una diversa entitï¿½ matematica:


\textbf{2)}Il corpo di un tf.Tensore ï¿½ il numero di elementi in ogni dimensione. TF automaticamente deduce il corpo durante la costruzione del grafico.
\textbf{3)} Il tipo ï¿½ il tipo di dato a cui appartongono gli elementi del tensore.%controllare che viene bene con i numeri


I principali tipi di tensori sono:
\begin{itemize}
\item Variabili (\textit{tf.Variable}): i parametri dell'algoritmo che verranno cambiati per ottimizzare l'algoritmo
\item Costanti (\textit{tf.constant})
\item Segnaposto(\textit{tf.placeholder}): consentono di inserire dati e di creare operazioni per costruire il grafico computazionale, possono dipendere da altri dati ad esempio il risultato previsto di un calcolo. Possono essere usati piï¿½ volte e non dare lo stesso risultato.
\item Tensore sparso (\textit{tf.SparseTensor})
\end{itemize}



\clearpage
\addcontentsline{toc}{chapter}{Bibliografia}
\bibliographystyle{unsrt}
\bibliography{bibliografia}
\nocite{deep,machine,imagenet}
\end{document}
